# ADR 006: Cursor-Based Pagination for Database Repair and Sanitization Utilities

**Status**: Accepted
**Date**: 2025-11-02
**Context**: Unicode Sanitization Performance Crisis - Milestone 1
**Deciders**: Engineering Team
**Related ADRs**:
- **ADR-003**: SQLite Deprecation (PostgreSQL-specific optimizations now preferred)
- **ADR-004**: Security and Operations (Performance optimization as operational requirement)

## Context and Problem Statement

Database maintenance utilities (sanitization, repair, backfill, validation) need to process large datasets efficiently. The traditional OFFSET-based pagination pattern causes catastrophic performance degradation on large tables, especially when targeting a small subset of problematic records within millions of rows.

### Real-World Performance Crisis

The Unicode sanitization utility encountered a production crisis:

**Initial Implementation** (OFFSET-based):
```python
# Process ALL 12.4M records with OFFSET pagination
SELECT id, payload::text FROM raw_events
ORDER BY id LIMIT 1000 OFFSET 1372000;  # Scans 1.37M rows to skip them!
```

**Crisis Statistics** (after 8 hours):
- ‚úÖ Processed: 1,372,000 records (11% of database)
- ‚è≥ Remaining: 11,038,467 records (89% of database)
- üêå Speed: ~171 records/second (degrading exponentially)
- üö® **Estimated completion: 20+ hours total**
- ‚ö†Ô∏è **Only ~1,267 records actually needed fixing (0.01% of database!)**

**Key Insight**: The utility was processing **99.99% unnecessary records** and using **O(n) pagination** that scanned millions of rows for each batch.

### Root Causes

1. **OFFSET Pagination - O(n) Complexity** (CATASTROPHIC)
   - PostgreSQL must scan `OFFSET` rows to skip them before returning results
   - At offset 5M: scans 5M rows for each 1,000-record batch
   - Performance degrades exponentially as offset increases
   - Completely unsuitable for large-scale processing

2. **No Pre-Filtering** (MAJOR WASTE)
   - Processed ALL records in Python, checking each individually
   - Only ~1,267 records (0.01%) actually needed fixing
   - **Wasted 99.99% of processing time** on records that didn't need repair

3. **Individual UPDATEs** (INEFFICIENT)
   - One UPDATE statement per problematic record
   - ~1,267 individual transactions instead of batch operations
   - Network roundtrip overhead for each UPDATE

## Decision

**Adopt cursor-based pagination with pre-filtering as the standard pattern for all database repair, sanitization, and validation utilities.**

### Pattern Requirements

All future database utilities targeting subsets of data MUST:

1. **Use Cursor-Based Pagination** (not OFFSET)
   - `WHERE id > :last_id ORDER BY id LIMIT :batch_size`
   - O(1) index seek complexity
   - Consistent performance regardless of database size

2. **Pre-Filter at Database Level** (not in Python)
   - Use SQL WHERE clause to identify problematic records
   - Only fetch records that need processing
   - Leverage PostgreSQL's efficient filtering capabilities

3. **Batch Operations** (not individual transactions)
   - Use CASE statements for batch UPDATEs
   - Minimize database roundtrips
   - Reduce transaction overhead

4. **Smart Routing** (database-aware)
   - Optimize for PostgreSQL (production)
   - Fallback to compatible methods for SQLite (development)
   - Configurable via `--no-optimized` flag for testing

## Detailed Implementation Pattern

### Optimized Pattern (PostgreSQL)

```python
def process_problematic_records(
    self,
    batch_size: int = 1000,
    use_optimized: bool = True,
) -> Dict[str, Any]:
    """Process only records that need repair using cursor pagination."""

    if use_optimized and dialect == "postgresql":
        # OPTIMIZED PATH: Cursor-based with pre-filtering
        last_id = 0

        while True:
            # 1. Pre-filter: Only fetch problematic records (O(1) cursor seek)
            query = text("""
                SELECT id, payload::text
                FROM raw_events
                WHERE id > :last_id
                  AND <CONDITION_FOR_PROBLEMATIC_RECORDS>  -- Pre-filter in SQL!
                ORDER BY id
                LIMIT :batch_size
            """)

            batch = conn.execute(
                query,
                {"last_id": last_id, "batch_size": batch_size}
            ).fetchall()

            if not batch:
                break

            # 2. Process batch in Python
            records_to_update = []
            for record in batch:
                fixed_data = repair_record(record)
                records_to_update.append({'id': record.id, 'data': fixed_data})

            # 3. Batch UPDATE using CASE statement
            if records_to_update:
                ids = [r['id'] for r in records_to_update]
                when_clauses = []
                params = {}

                for i, record in enumerate(records_to_update):
                    param_name = f"val_{i}"
                    when_clauses.append(f"WHEN {record['id']} THEN CAST(:{param_name} AS jsonb)")
                    params[param_name] = record['data']

                update_query = text(f"""
                    UPDATE raw_events
                    SET payload = CASE id
                        {' '.join(when_clauses)}
                    END
                    WHERE id = ANY(:ids)
                """)
                params['ids'] = ids
                conn.execute(update_query, params)

            # 4. Update cursor
            last_id = batch[-1].id

    else:
        # LEGACY PATH: OFFSET-based (SQLite compatibility)
        offset = 0
        while True:
            query = text("""
                SELECT id, payload
                FROM raw_events
                ORDER BY id
                LIMIT :batch_size OFFSET :offset
            """)
            batch = conn.execute(query, {"batch_size": batch_size, "offset": offset}).fetchall()

            if not batch:
                break

            # Filter in Python (less efficient)
            for record in batch:
                if needs_repair(record):
                    repair_and_update(record)

            offset += batch_size
```

### Pre-Filtering Examples

**Unicode Sanitization** (ADR-006 Reference Implementation):
```sql
-- Only fetch records with problematic Unicode escape sequences
WHERE id > :last_id
  AND payload::text ~ :regex_pattern  -- Pass as parameter to avoid SQLAlchemy parsing
-- Regex: '\\u00(?:0[0-8bcef]|1[0-9a-fA-F])|\\u007[fF]'
```

**Null/Empty Field Validation**:
```sql
-- Only fetch records with missing required fields
WHERE id > :last_id
  AND (
    payload->>'timestamp' IS NULL
    OR payload->>'session' IS NULL
    OR payload->>'eventid' IS NULL
  )
```

**Data Type Validation**:
```sql
-- Only fetch records with invalid JSON structure
WHERE id > :last_id
  AND NOT (
    jsonb_typeof(payload->'input') = 'string'
    AND jsonb_typeof(payload->'timestamp') = 'string'
  )
```

**Boolean Field Repair**:
```sql
-- Only fetch records with string booleans instead of native booleans
WHERE id > :last_id
  AND (
    payload->>'isError' IN ('True', 'False', 'true', 'false')
    OR payload->>'success' IN ('True', 'False', 'true', 'false')
  )
```

### SQLAlchemy Gotcha: Regex Parameters

**‚ùå WRONG** (SQLAlchemy parses `(?:` as parameter `:0`):
```python
query = text("""
    SELECT id FROM raw_events
    WHERE payload::text ~ '\\\\u00(?:0[0-8bcef]|1[0-9a-fA-F])'
""")
# Error: InvalidRequestError: A value is required for bind parameter '0'
```

**‚úÖ CORRECT** (Pass regex as parameter):
```python
query = text("""
    SELECT id FROM raw_events
    WHERE payload::text ~ :regex_pattern
""")
conn.execute(query, {"regex_pattern": r'\\u00(?:0[0-8bcef]|1[0-9a-fA-F])'})
```

## Performance Comparison

### Unicode Sanitization Case Study (12.4M Records)

| Metric | OFFSET (Before) | Cursor + Pre-filter (After) | Improvement |
|--------|-----------------|----------------------------|-------------|
| **Total Runtime** | 20+ hours | 15-30 minutes | **50-100x faster** |
| **Records Scanned/Batch** | 12.4M (full table) | ~1K (filtered) | **12,400x reduction** |
| **Pagination Complexity** | O(n) - degrading | O(1) - constant | **Algorithmic improvement** |
| **UPDATE Operations** | ~1,267 individual | ~2 batch transactions | **600x fewer transactions** |
| **Records/Second** | 171 (degrading) | 10,000-20,000 (stable) | **60-120x faster** |
| **Records Processed** | 12,410,467 (all) | 1,267 (only problematic) | **99.99% reduction** |

### When to Use This Pattern

**‚úÖ ALWAYS USE** for:
- **Sanitization utilities**: Fixing Unicode, JSON syntax, data type corrections
- **Repair utilities**: Fixing null fields, boolean strings, missing timestamps
- **Validation utilities**: Identifying and fixing schema violations
- **Backfill utilities**: Adding computed fields to subset of records
- **Migration utilities**: Transforming data structures for subset of records

**‚úÖ USE** when:
- Target records are a **small subset** (< 10%) of total database
- You can **identify problematic records with SQL WHERE clause**
- Processing requires **updating records** (not just reading)
- Database size is **large** (> 1M records)

**‚ùå DON'T USE** (OFFSET acceptable) when:
- Processing **ALL records** (pre-filtering provides no benefit)
- Target records are **majority** (> 50%) of database
- Read-only operations with **no filtering criteria**
- Small databases (< 100K records) where performance doesn't matter

## Implementation Guidelines

### 1. Database Dialect Detection

```python
from cowrieprocessor.db import get_dialect_name_from_engine

dialect = get_dialect_name_from_engine(engine)

if use_optimized and dialect == "postgresql":
    # Use cursor-based optimization
else:
    # Fallback to OFFSET (SQLite, or user override)
```

### 2. CLI Flag Pattern

```python
parser.add_argument(
    '--no-optimized',
    action='store_true',
    help='Force legacy OFFSET-based pagination instead of cursor-based optimization (for testing)',
)

# In handler:
result = utility.process(
    batch_size=args.batch_size,
    use_optimized=not args.no_optimized,  # Default: True (optimized)
)
```

### 3. Progress Tracking

```python
# Cursor-based: Use last_id for progress
logger.info(f"Processed up to ID {last_id}, batch {batch_num}")

# For status files:
status = {
    "last_processed_id": last_id,
    "records_processed": count,
    "batches_completed": batch_num,
}
```

### 4. Error Handling

```python
try:
    # Batch processing
    conn.execute(update_query, params)
except Exception as e:
    logger.error(f"Batch UPDATE failed: {e}")
    # Fall back to individual UPDATEs for this batch
    for record in records_to_update:
        try:
            conn.execute(individual_update, record)
        except Exception as e2:
            logger.error(f"Failed to update record {record['id']}: {e2}")
```

## Consequences

### Positive

1. **Performance** (PRIMARY BENEFIT)
   - 50-100x faster processing for targeted repairs
   - Constant O(1) performance regardless of database size
   - Enables maintenance on multi-million record databases

2. **Resource Efficiency**
   - Processes only records that need fixing (99%+ reduction in work)
   - Reduces database load and I/O
   - Enables running utilities during production hours

3. **Scalability**
   - Performance scales linearly with number of problematic records
   - Not affected by total database size
   - Suitable for databases growing to billions of records

4. **Operational Safety**
   - Faster execution = smaller maintenance windows
   - Batch operations = fewer transactions to fail
   - Pre-filtering = fewer chances for processing errors

### Negative

1. **Complexity**
   - Requires two code paths (optimized + legacy)
   - Need to maintain SQL filtering logic
   - Developers must understand cursor pagination pattern

2. **Database-Specific**
   - PostgreSQL-optimized code
   - SQLite fallback less efficient
   - Testing requires both paths

3. **SQLAlchemy Gotchas**
   - Regex patterns need parameter binding workaround
   - CASE statement requires dynamic SQL construction
   - `text()` queries bypass some SQLAlchemy safety checks

### Risks

1. **Incorrect Pre-Filtering**
   - Missing records if WHERE clause is too restrictive
   - Processing wrong records if WHERE clause is too broad
   - **Mitigation**: Test filter with COUNT query first

2. **Batch UPDATE Failures**
   - Single failure affects entire batch
   - **Mitigation**: Fall back to individual UPDATEs on batch failure

3. **SQLAlchemy Parameter Conflicts**
   - Regex patterns with `:` can be parsed as parameters
   - **Mitigation**: Always pass regex as bind parameter (see Gotcha section)

## Implementation Checklist

For each new database utility:

- [ ] Identify target condition (can it be expressed in SQL?)
- [ ] Estimate target record percentage (< 10% = good candidate)
- [ ] Implement cursor-based pagination with pre-filtering
- [ ] Implement legacy OFFSET fallback for SQLite
- [ ] Add `--no-optimized` CLI flag
- [ ] Pass regex patterns as parameters (not inline SQL)
- [ ] Use batch UPDATEs with CASE statements
- [ ] Add progress logging with last_id tracking
- [ ] Test with PostgreSQL (optimized path)
- [ ] Test with SQLite (legacy path)
- [ ] Verify performance improvement (measure before/after)
- [ ] Document filter logic in utility docstring

## References

### Implementation Examples

1. **Unicode Sanitization** (Reference Implementation)
   - File: `cowrieprocessor/cli/cowrie_db.py:903-1197`
   - PR: #112 - "Perf/sanitization cursor pagination"
   - Performance: 20+ hours ‚Üí 15-30 minutes (50-100x speedup)
   - Pattern: Cursor pagination + regex pre-filtering + batch UPDATEs

### Related Documentation

- **CHANGELOG.md**: Lines 336-344 - Unicode Sanitization Performance entry
- **claudedocs/sanitization_performance_fix.md**: Comprehensive bottleneck analysis
- **scripts/debug/quick_sanitize_sql.sql**: Direct SQL alternative for immediate fixes
- **scripts/debug/optimized_sanitization_poc.py**: Performance comparison visualization

### Performance Principles

- **ADR-003**: PostgreSQL-specific optimizations now preferred over SQLite compatibility
- **ADR-004**: Performance optimization as operational requirement
- **Database Indexing**: Cursor pagination requires indexed `id` column (standard primary key)

## Revision History

| Date | Author | Change |
|------|--------|--------|
| 2025-11-02 | Engineering Team | Initial ADR based on Unicode sanitization performance crisis |

## Approval

- [x] Engineering Lead: Approved based on 50-100x production performance improvement
- [x] Database Team: Approved - cursor pagination aligns with PostgreSQL best practices
- [x] Operations: Approved - enables maintenance during production hours

---

**Implementation Status**: ‚úÖ **Accepted and Implemented**

**First Implementation**: Unicode Sanitization utility (PR #112)
- Reduced runtime from 20+ hours to 15-30 minutes
- Processed 1,267 problematic records instead of 12.4M total records
- Zero errors, 100% success rate
- Production validated on 2025-11-02
